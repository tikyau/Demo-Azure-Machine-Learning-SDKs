{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "- [Load workspace](#Load-workspace)\n",
    "- [Create / connect to an experiment](#Create-/-connect-to-an-experiment)\n",
    "- [Upload data files into datastore](#Upload-data-files-into-datastore)\n",
    "- [Create training scripts](#Create-training-scripts)\n",
    "- [Create / connect to a remote compute target](#Create-/-connect-to-a-remote-compute-target)\n",
    "- [Create an estimator](#Create-an-estimator)\n",
    "- [Submit the job to the cluster & Run](#Submit-the-job-to-the-cluster-&-Run)\n",
    "- [Register model](#Register-model)\n",
    "- [Clean up the compute target](#Clean-up-the-compute-target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  0.1.59\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import azureml\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /home/nbuser/library/aml_config/config.json\n",
      "Xiangzhe-WS\twesteurope\tXiangzhe-ML\twesteurope\n"
     ]
    }
   ],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create / connect to an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experiment\n",
    "experiment_name = 'nyc-taxi-batch-AI'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace = ws, name = experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data files into datastore\n",
    "\n",
    "Every workspace comes with a default datastore which is backed by the Azure blob storage account associated with the workspace. We can use it to transfer data from local to the cloud, and access it from the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureFile xiangzhews1068013949 azureml-filestore-bc063c69-64a6-48ce-90f5-33cb3c8d43b2\n"
     ]
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "print(ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_5fe09e9af7b443fa8d09db5c1b02df2c"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload_files(['./data_after_prep.pkl'], target_path='nyc-taxi', overwrite=True, show_progress=True)\n",
    "#ds.upload(src_dir='.', target_path='nyc-taxi', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training scripts\n",
    "\n",
    "### Create a script directory\n",
    "\n",
    "Create a directory to deliver the necessary code from local to the remote target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './scripts_batch_AI'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create scripts\n",
    "\n",
    "To submit the job to the cluster, we should create a training script.\n",
    "\n",
    "_**Note**: The data path settings of DSVM and Batch AI cluster are different. Be careful !!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scripts_batch_AI/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_submitted_run()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = os.path.join(args.data_folder, 'nyc-taxi')\n",
    "run.log('Data folder', data_folder)\n",
    "\n",
    "data_path = os.path.join(data_folder, 'data_after_prep.pkl')\n",
    "run.log('Data path', data_path)\n",
    "\n",
    "# load train and test set into numpy arrays\n",
    "pd_dataframe = pd.read_pickle(data_path)\n",
    "run.log('Data loading', 'finished')\n",
    "\n",
    "# data processing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit([\"N\", \"Y\"])\n",
    "pd_dataframe[\"store_and_fwd_flag\"] = le.transform(pd_dataframe[\"store_and_fwd_flag\"])\n",
    "\n",
    "le.fit([\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"])\n",
    "pd_dataframe[\"pickup_weekday\"] = le.transform(pd_dataframe[\"pickup_weekday\"])\n",
    "pd_dataframe[\"dropoff_weekday\"] = le.transform(pd_dataframe[\"dropoff_weekday\"])\n",
    "run.log('Data processing', 'finished')\n",
    "\n",
    "# load dataset into numpy arrays\n",
    "y = np.array(pd_dataframe[\"trip_duration\"]).astype(float)\n",
    "y = np.log(y)\n",
    "X = np.array(pd_dataframe.drop([\"trip_duration\"],axis = 1))\n",
    "\n",
    "# normalize data\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "run.log('Normalization', 'finished')\n",
    "\n",
    "# split data into train and validation datasets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.25, random_state = 20)\n",
    "\n",
    "# train LR model\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "run.log('Model training', 'finished')\n",
    "\n",
    "y_pred = lm.predict(X_val)\n",
    "run.log('Prediction', 'finished')\n",
    "\n",
    "# evaluation\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "run.log('Evaluation', 'finished')\n",
    "run.log('Mean Squared Error', np.float(mse))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note!!! file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=lm, filename='outputs/nyc_taxi_model_cluster.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create / connect to a remote compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target traincluster, just use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, BatchAiCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "batchai_cluster_name = \"traincluster\"\n",
    "\n",
    "try:\n",
    "    # look for the existing cluster by name\n",
    "    compute_target = ComputeTarget(workspace=ws, name=batchai_cluster_name)\n",
    "    if type(compute_target) is BatchAiCompute:\n",
    "        print('found compute target {}, just use it.'.format(batchai_cluster_name))\n",
    "    else:\n",
    "        print('{} exists but it is not a Batch AI cluster. Please choose a different name.'.format(batchai_cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print('creating a new compute target...')\n",
    "    compute_config = BatchAiCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\", # small CPU-based VM\n",
    "                                                                #vm_priority='lowpriority', # optional\n",
    "                                                                autoscale_enabled=True,\n",
    "                                                                cluster_min_nodes=0, \n",
    "                                                                cluster_max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, batchai_cluster_name, compute_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "    # Use the 'status' property to get a detailed status for the current cluster. \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an estimator\n",
    "\n",
    "An estimator object is used to submit the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount()\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['numpy','pandas','scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the job to the cluster & Run\n",
    "\n",
    "Run the experiment by submitting the estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>nyc-taxi-batch-AI</td><td>nyc-taxi-batch-AI_1538578635582</td><td>azureml.scriptrun</td><td>Running</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/97fdb0ec-6341-4370-a234-394b1581d86c/resourceGroups/Xiangzhe-ML/providers/Microsoft.MachineLearningServices/workspaces/Xiangzhe-WS/experiments/nyc-taxi-batch-AI/runs/nyc-taxi-batch-AI_1538578635582\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: nyc-taxi-batch-AI,\n",
       "Id: nyc-taxi-batch-AI_1538578635582,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Running)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(config = est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show running details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79890b7817c54643a237674e7fef0802",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRun()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MSI: Failed to retrieve a token from 'http://localhost:25198/nb/api/nbsvc/oauth2/token' with an error of 'HTTPConnectionPool(host='localhost', port=25198): Max retries exceeded with url: /nb/api/nbsvc/oauth2/token (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f16d330dd68>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))'. This could be caused by the MSI extension not yet fullly provisioned.\n",
      "MSI: Failed to retrieve a token from 'http://localhost:25198/nb/api/nbsvc/oauth2/token' with an error of 'HTTPConnectionPool(host='localhost', port=25198): Max retries exceeded with url: /nb/api/nbsvc/oauth2/token (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f16d330d630>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))'. This could be caused by the MSI extension not yet fullly provisioned.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display run results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data folder': '/mnt/batch/tasks/shared/LS_root/jobs/traincluster8575340143/azureml/nyc-taxi-batch-ai_1538578635582/mounts/workspacefilestore/nyc-taxi', 'Data path': '/mnt/batch/tasks/shared/LS_root/jobs/traincluster8575340143/azureml/nyc-taxi-batch-ai_1538578635582/mounts/workspacefilestore/nyc-taxi/data_after_prep.pkl', 'Data loading': 'finished', 'Data processing': 'finished', 'Normalization': 'finished', 'Model training': 'finished', 'Prediction': 'finished', 'Evaluation': 'finished', 'Mean Squared Error': 0.3878969301600042}\n"
     ]
    }
   ],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "`outputs` is a special directory in that all content in this directory is automatically uploaded to your workspace. Hence, the model file will also available in the workspace.\n",
    "\n",
    "We can see files associated with that run with the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'outputs/nyc_taxi_model_cluster.pkl', 'driver_log', 'azureml-logs/azureml.log', 'azureml-logs/55_batchai_execution.txt']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in the workspace so that you (or other collaborators) can later query, examine, and deploy this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nyc_taxi_model_cluster\tnyc_taxi_model_cluster:1\t1\n"
     ]
    }
   ],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='nyc_taxi_model_cluster', model_path='outputs/nyc_taxi_model_cluster.pkl')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally, delete the Azure Managed Compute cluster\n",
    "compute_target.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
